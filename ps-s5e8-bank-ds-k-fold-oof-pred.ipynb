{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# This is Stacked Ensemble Pipeline\n",
    "#\n",
    "# This script implements a full K-fold oof approach:\n",
    "# 1. Hyperparameter Tuning: Uses Optuna to find the best parameters\n",
    "#    for each base model via K-Fold Cross-Validation.\n",
    "# 2. Stacking: Generates Out-of-Fold (OOF) predictions using the\n",
    "#    best parameters to train a meta-model.\n",
    "# 3. Final Training: Retrains the base models on the ENTIRE\n",
    "#    training dataset and uses the meta-model to predict on the\n",
    "#    test set for the final submission.\n",
    "# ==================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# Importing libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-08-17T05:11:49.973305Z",
     "iopub.status.busy": "2025-08-17T05:11:49.972997Z",
     "iopub.status.idle": "2025-08-17T05:11:57.239503Z",
     "shell.execute_reply": "2025-08-17T05:11:57.238710Z",
     "shell.execute_reply.started": "2025-08-17T05:11:49.973277Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Optional These are optional and used for parameter tunning if Not found then manually set params used .\n",
    "try:\n",
    "    import optuna\n",
    "    HAS_OPTUNA = True\n",
    "except ImportError:\n",
    "    HAS_OPTUNA = False\n",
    "    print(\"Optuna not found.\")\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostClassifier, Pool\n",
    "    HAS_CATBOOST = True\n",
    "except ImportError:\n",
    "    HAS_CATBOOST = False\n",
    "    print(\"CatBoost not found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# Loading Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-08-17T05:11:57.241051Z",
     "iopub.status.busy": "2025-08-17T05:11:57.240376Z",
     "iopub.status.idle": "2025-08-17T05:12:00.469588Z",
     "shell.execute_reply": "2025-08-17T05:12:00.468659Z",
     "shell.execute_reply.started": "2025-08-17T05:11:57.241025Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading and preparing data...\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 1) Load and Prepare Data\n",
    "# -------------------------------\n",
    "TRAIN_PATH = '/kaggle/input/ps-s5e8/train.csv'\n",
    "TEST_PATH = '/kaggle/input/ps-s5e8/test.csv'\n",
    "SAMPLE_SUB_PATH = '/kaggle/input/ps-s5e8/sample_submission.csv'\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "test_df = pd.read_csv(TEST_PATH)\n",
    "sample_sub = pd.read_csv(SAMPLE_SUB_PATH)\n",
    "\n",
    "TARGET = \"y\"\n",
    "if TARGET not in train_df.columns:\n",
    "    raise ValueError(f\"Target column '{TARGET}' not found in train.csv\")\n",
    "\n",
    "y = train_df[TARGET].astype(int)\n",
    "X_full = train_df.drop(columns=[TARGET])\n",
    "X_test_full = test_df.copy()\n",
    "\n",
    "# Align columns - crucial for consistency\n",
    "train_cols = X_full.columns\n",
    "test_cols = X_test_full.columns\n",
    "shared_cols = list(set(train_cols) & set(test_cols))\n",
    "X_full = X_full[shared_cols]\n",
    "X_test_full = X_test_full[shared_cols]\n",
    "\n",
    "id_col = next((col for col in [\"id\", \"ID\", \"Id\"] if col in test_df.columns), None)\n",
    "sub_target_col = sample_sub.columns[1] if id_col else sample_sub.columns[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# Preprocessing the data,FE, handling missing values and Encoding columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-08-17T05:12:00.472340Z",
     "iopub.status.busy": "2025-08-17T05:12:00.472006Z",
     "iopub.status.idle": "2025-08-17T05:12:07.827063Z",
     "shell.execute_reply": "2025-08-17T05:12:07.826144Z",
     "shell.execute_reply.started": "2025-08-17T05:12:00.472316Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Step 2: Preprocessing data...\")\n",
    "cat_cols = [c for c in X_full.columns if X_full[c].dtype == \"object\"]\n",
    "num_cols = [c for c in X_full.columns if c not in cat_cols]\n",
    "\n",
    "for c in num_cols:\n",
    "    med = X_full[c].median()\n",
    "    X_full[c] = X_full[c].fillna(med)\n",
    "    X_test_full[c] = X_test_full[c].fillna(med)\n",
    "\n",
    "for c in cat_cols:\n",
    "    mode = X_full[c].mode()[0]\n",
    "    X_full[c] = X_full[c].astype(\"category\").cat.add_categories([\"__MISSING__\"]).fillna(\"__MISSING__\")\n",
    "    X_test_full[c] = X_test_full[c].astype(\"category\").cat.add_categories([\"__MISSING__\"]).fillna(\"__MISSING__\")\n",
    "\n",
    "def kfold_target_encode(train_series, target, test_series, n_splits=5, smoothing=20):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    oof = np.zeros(len(train_series), dtype=float)\n",
    "    test_encoded = np.zeros(len(test_series), dtype=float)\n",
    "    global_mean = target.mean()\n",
    "\n",
    "    for tr_idx, va_idx in skf.split(train_series, target):\n",
    "        tr_vals, va_vals = train_series.iloc[tr_idx], train_series.iloc[va_idx]\n",
    "        tr_target = target.iloc[tr_idx]\n",
    "        \n",
    "        counts = tr_vals.value_counts()\n",
    "        means = tr_vals.groupby(tr_vals).apply(lambda s: tr_target.loc[s.index].mean())\n",
    "        smooth = (means * counts + global_mean * smoothing) / (counts + smoothing)\n",
    "        \n",
    "        oof[va_idx] = va_vals.map(smooth).fillna(global_mean).values\n",
    "        test_encoded += test_series.map(smooth).fillna(global_mean).values / n_splits\n",
    "        \n",
    "    return oof, test_encoded\n",
    "\n",
    "X_enc = X_full[num_cols].copy()\n",
    "X_test_enc = X_test_full[num_cols].copy()\n",
    "\n",
    "for c in cat_cols:\n",
    "    tr_enc, te_enc = kfold_target_encode(X_full[c], y, X_test_full[c])\n",
    "    X_enc[f\"TE_{c}\"] = tr_enc\n",
    "    X_test_enc[f\"TE_{c}\"] = te_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# Parameters tunning finding best parameter by testing multiple combination using optuna.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "execution_failed": "2025-08-17T07:03:10.504Z",
     "iopub.execute_input": "2025-08-17T05:13:47.344002Z",
     "iopub.status.busy": "2025-08-17T05:13:47.343659Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nPART 1: HYPERPARAMETER TUNING WITH OPTUNA\")\n",
    "NFOLDS_TUNE = 3 \n",
    "N_TRIALS = 20 \n",
    "\n",
    "def xgb_objective(trial):\n",
    "    params = {\n",
    "        'objective': 'binary:logistic', 'eval_metric': 'auc', 'tree_method': 'gpu_hist',\n",
    "        'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
    "        'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
    "        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]),\n",
    "        'subsample': trial.suggest_categorical('subsample', [0.6, 0.7, 0.8, 1.0]),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "        'n_estimators': 1000,\n",
    "        'max_depth': trial.suggest_categorical('max_depth', [3, 4, 5, 6, 7, 8]),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 20),\n",
    "        'gamma': trial.suggest_float('gamma', 0.0, 1.0),\n",
    "    }\n",
    "    skf = StratifiedKFold(n_splits=NFOLDS_TUNE, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    for tr_idx, va_idx in skf.split(X_enc, y):\n",
    "        Xtr, Xva = X_enc.iloc[tr_idx], X_enc.iloc[va_idx]\n",
    "        ytr, yva = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "        model = xgb.XGBClassifier(**params, early_stopping_rounds=100, random_state=42)\n",
    "        model.fit(Xtr, ytr, eval_set=[(Xva, yva)], verbose=False)\n",
    "        preds = model.predict_proba(Xva)[:, 1]\n",
    "        scores.append(roc_auc_score(yva, preds))\n",
    "    return np.mean(scores)\n",
    "\n",
    "def lgb_objective(trial):\n",
    "    params = {\n",
    "        'objective': 'binary', 'metric': 'auc', 'boosting_type': 'gbdt',\n",
    "        'n_estimators': 1000,\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),\n",
    "    }\n",
    "    skf = StratifiedKFold(n_splits=NFOLDS_TUNE, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    for tr_idx, va_idx in skf.split(X_enc, y):\n",
    "        Xtr, Xva = X_enc.iloc[tr_idx], X_enc.iloc[va_idx]\n",
    "        ytr, yva = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "        model = lgb.LGBMClassifier(**params, random_state=42)\n",
    "        model.fit(Xtr, ytr, eval_set=[(Xva, yva)], callbacks=[lgb.early_stopping(100, verbose=False)])\n",
    "        preds = model.predict_proba(Xva)[:, 1]\n",
    "        scores.append(roc_auc_score(yva, preds))\n",
    "    return np.mean(scores)\n",
    "\n",
    "if HAS_OPTUNA:\n",
    "    print(f\"Tuning XGBoost ({N_TRIALS} trials)...\")\n",
    "    study_xgb = optuna.create_study(direction='maximize')\n",
    "    study_xgb.optimize(xgb_objective, n_trials=N_TRIALS)\n",
    "    xgb_best_params = study_xgb.best_params\n",
    "    print(\"Best XGBoost Params:\", xgb_best_params)\n",
    "\n",
    "    print(f\"\\nTuning LightGBM ({N_TRIALS} trials)...\")\n",
    "    study_lgb = optuna.create_study(direction='maximize')\n",
    "    study_lgb.optimize(lgb_objective, n_trials=N_TRIALS)\n",
    "    lgb_best_params = study_lgb.best_params\n",
    "    print(\"Best LightGBM Params:\", lgb_best_params)\n",
    "    \n",
    "else:\n",
    "    # This params are gonna used ifparameters if Optuna is not available\n",
    "    xgb_best_params = {'lambda': 1.5, 'alpha': 1.5, 'colsample_bytree': 0.7, 'subsample': 0.8, 'learning_rate': 0.02, 'max_depth': 6, 'min_child_weight': 1, 'gamma': 0.1}\n",
    "    lgb_best_params = {'learning_rate': 0.02, 'num_leaves': 64, 'max_depth': 7, 'min_child_samples': 20, 'subsample': 0.8, 'colsample_bytree': 0.8, 'reg_alpha': 0.1, 'reg_lambda': 0.1}\n",
    "\n",
    "if HAS_CATBOOST:\n",
    "    cat_best_params = {'learning_rate': 0.03, 'depth': 6, 'l2_leaf_reg': 3.0, 'loss_function': 'Logloss', 'eval_metric': 'AUC', 'task_type': 'GPU'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# Testing the oof predictions on stacked dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.status.busy": "2025-08-17T05:12:09.034877Z",
     "iopub.status.idle": "2025-08-17T05:12:09.035269Z",
     "shell.execute_reply": "2025-08-17T05:12:09.035153Z",
     "shell.execute_reply.started": "2025-08-17T05:12:09.035139Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\nPART 2: GENERATING OUR OOF PREDICTIONS FOR STACKING\")\n",
    "NFOLDS_STACK = 5\n",
    "skf = StratifiedKFold(n_splits=NFOLDS_STACK, shuffle=True, random_state=42)\n",
    "\n",
    "oof_xgb = np.zeros(len(X_enc))\n",
    "oof_lgb = np.zeros(len(X_enc))\n",
    "oof_cat = np.zeros(len(X_enc)) if HAS_CATBOOST else None\n",
    "\n",
    "best_iter_xgb, best_iter_lgb, best_iter_cat = [], [], []\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(skf.split(X_enc, y), 1):\n",
    "    print(f\"===== Stacking Fold {fold}/{NFOLDS_STACK} =====\")\n",
    "    Xtr_enc, Xva_enc = X_enc.iloc[tr_idx], X_enc.iloc[va_idx]\n",
    "    ytr, yva = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "\n",
    "    xgb_model = xgb.XGBClassifier(**xgb_best_params, n_estimators=2000, early_stopping_rounds=200, random_state=42)\n",
    "    xgb_model.fit(Xtr_enc, ytr, eval_set=[(Xva_enc, yva)], verbose=False)\n",
    "    oof_xgb[va_idx] = xgb_model.predict_proba(Xva_enc)[:, 1]\n",
    "    best_iter_xgb.append(xgb_model.best_iteration)\n",
    "    print(f\"  XGB Fold AUC: {roc_auc_score(yva, oof_xgb[va_idx]):.6f}\")\n",
    "\n",
    "    lgb_model = lgb.LGBMClassifier(**lgb_best_params, n_estimators=2000, random_state=42)\n",
    "    lgb_model.fit(Xtr_enc, ytr, eval_set=[(Xva_enc, yva)], callbacks=[lgb.early_stopping(200, verbose=False)])\n",
    "    oof_lgb[va_idx] = lgb_model.predict_proba(Xva_enc)[:, 1]\n",
    "    # FIX: Use .best_iteration_ for the scikit-learn API\n",
    "    best_iter_lgb.append(lgb_model.best_iteration_)\n",
    "    print(f\"  LGB Fold AUC: {roc_auc_score(yva, oof_lgb[va_idx]):.6f}\")\n",
    "    \n",
    "    if HAS_CATBOOST:\n",
    "        cat_model = CatBoostClassifier(**cat_best_params, iterations=3000, od_type=\"Iter\", od_wait=300, random_seed=42, verbose=0)\n",
    "        cat_model.fit(X_full.iloc[tr_idx], ytr, eval_set=(X_full.iloc[va_idx], yva), cat_features=cat_cols)\n",
    "        oof_cat[va_idx] = cat_model.predict_proba(X_full.iloc[va_idx])[:, 1]\n",
    "        best_iter_cat.append(cat_model.get_best_iteration())\n",
    "        print(f\"  CAT Fold AUC: {roc_auc_score(yva, oof_cat[va_idx]):.6f}\")\n",
    "        \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# main model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.status.busy": "2025-08-17T05:12:09.036080Z",
     "iopub.status.idle": "2025-08-17T05:12:09.036354Z",
     "shell.execute_reply": "2025-08-17T05:12:09.036240Z",
     "shell.execute_reply.started": "2025-08-17T05:12:09.036228Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\nPART 3: TRAINING META-MODEL ON OOF PREDICTIONS\")\n",
    "if HAS_CATBOOST:\n",
    "    oof_stack = np.vstack([oof_xgb, oof_lgb, oof_cat]).T\n",
    "else:\n",
    "    oof_stack = np.vstack([oof_xgb, oof_lgb]).T\n",
    "\n",
    "meta_model = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"lr\", LogisticRegression(solver=\"liblinear\", random_state=42))\n",
    "])\n",
    "meta_model.fit(oof_stack, y)\n",
    "meta_oof_auc = roc_auc_score(y, meta_model.predict_proba(oof_stack)[:, 1])\n",
    "print(f\"Meta-Model OOF AUC: {meta_oof_auc:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# Estimating finale prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.status.busy": "2025-08-17T05:12:09.037740Z",
     "iopub.status.idle": "2025-08-17T05:12:09.038061Z",
     "shell.execute_reply": "2025-08-17T05:12:09.037904Z",
     "shell.execute_reply.started": "2025-08-17T05:12:09.037892Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\nPART 4: TRAINING FINAL MODELS ON FULL DATASET\")\n",
    "\n",
    "n_est_xgb = int(np.mean(best_iter_xgb) * 1.1)\n",
    "n_est_lgb = int(np.mean(best_iter_lgb) * 1.1)\n",
    "print(f\"Final rounds -> XGB: {n_est_xgb}, LGB: {n_est_lgb}\")\n",
    "\n",
    "print(\"Training final XGBoost model...\")\n",
    "final_xgb = xgb.XGBClassifier(**xgb_best_params, n_estimators=n_est_xgb, random_state=42)\n",
    "final_xgb.fit(X_enc, y, verbose=False)\n",
    "test_pred_xgb = final_xgb.predict_proba(X_test_enc)[:, 1]\n",
    "\n",
    "print(\"Training final LightGBM model...\")\n",
    "final_lgb = lgb.LGBMClassifier(**lgb_best_params, n_estimators=n_est_lgb, random_state=42)\n",
    "final_lgb.fit(X_enc, y)\n",
    "test_pred_lgb = final_lgb.predict_proba(X_test_enc)[:, 1]\n",
    "\n",
    "if HAS_CATBOOST:\n",
    "    n_est_cat = int(np.mean(best_iter_cat) * 1.1)\n",
    "    print(f\"Final rounds -> CAT: {n_est_cat}\")\n",
    "    print(\"Training final CatBoost model...\")\n",
    "    final_cat = CatBoostClassifier(**cat_best_params, iterations=n_est_cat, random_seed=42, verbose=0)\n",
    "    final_cat.fit(X_full, y, cat_features=cat_cols)\n",
    "    test_pred_cat = final_cat.predict_proba(X_test_full)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.status.busy": "2025-08-17T05:12:09.039519Z",
     "iopub.status.idle": "2025-08-17T05:12:09.039845Z",
     "shell.execute_reply": "2025-08-17T05:12:09.039686Z",
     "shell.execute_reply.started": "2025-08-17T05:12:09.039669Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\nPART 5: CREATING FINAL SUBMISSION\")\n",
    "\n",
    "if HAS_CATBOOST:\n",
    "    test_stack_full = np.vstack([test_pred_xgb, test_pred_lgb, test_pred_cat]).T\n",
    "else:\n",
    "    test_stack_full = np.vstack([test_pred_xgb, test_pred_lgb]).T\n",
    "\n",
    "# Use the meta-model to make the final prediction\n",
    "# meta model is our master model which uses the oof predictions made by other models and learns from that then predicts y\n",
    "final_predictions = meta_model.predict_proba(test_stack_full)[:, 1]\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "if id_col:\n",
    "    submission[id_col] = test_df[id_col]\n",
    "submission[sub_target_col] = final_predictions\n",
    "submission.to_csv(\"submission_final.csv\", index=False)\n",
    "\n",
    "print(\"\\nsubmission_final.csv created successfully!\")\n",
    "print(\"Pipeline finished.\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 12937777,
     "sourceId": 91719,
     "sourceType": "competition"
    },
    {
     "datasetId": 8079816,
     "sourceId": 12780204,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
